{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 141872 characters, 91 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('mega_poem.txt', 'r', encoding=\"utf-8\").read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text[:], block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/22/2021 09:29:44 - INFO - mingpt.model -   number of parameters: 2.537882e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)\n",
    "torch.save(model.state_dict(), \"sample_model2.checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1107: train loss 0.24770. lr 3.001330e-04: 100%|██████████| 1108/1108 [05:52<00:00,  3.14it/s]\n",
      "11/22/2021 09:35:39 - INFO - mingpt.trainer -   saving latest_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=1, batch_size=128, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=0, ckpt_path=\"latest_model.ckpt\")\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem Start\n",
      "I Am A Sculptor, A Molder Of Form\n",
      " \n",
      "I am a sculptor, a molder of form.\n",
      "In every moment I shape an idol.\n",
      "But then, in front of you, I melt them down\n",
      "I can rouse a hundred forms\n",
      "and fill them with spirit,\n",
      "but when I look into your face,\n",
      "I want to throw them in the fire.\n",
      "My body b is a secret tearted fills from the new sciences,\n",
      "the perfull of a mine that face? Should it manifest itself,\n",
      "out of desire for it forthwith maternal aunt would be estranged from my lips.\n",
      "“Then anger that in every moment you are a new lament hands he desert.\n",
      "Last night in the secret of the carn's no secret in the garden.\n",
      "In the shadows of paradise me undersand silence,\n",
      "so that seeks we shall treen and pen the straightforwards.\n",
      "I have a distracted birds\n",
      "can's discined from the snare,\n",
      "As fill the Creen, and then np thief.\n",
      "END\n",
      "Mewlana Jalaluddin Rumi\n",
      "\n",
      "\n",
      "Poem Start\n",
      "This We Have Now\n",
      " \n",
      "This we have now\n",
      "is not imagination.\n",
      "This is not\n",
      "grief or joy.\n",
      "Not a judging state of elation,\n",
      "our sadness.\n",
      "You are the called do not friends,\n",
      "And no thy Pot Eden of the Aaming Source of Thousand Years,\n",
      "END\n",
      "Omar Khayyam\n",
      "\n",
      "\n",
      "Poem Start\n",
      "LXII\n",
      "\n",
      "Love ! somet the Cartor wash aught\n",
      "Do with althought Surc—thc God\n",
      "Whcre shall the Sun in Turn shall A—Noice!\n",
      "END\n",
      "Omar Khayyam\n",
      "\n",
      "\n",
      "Poem Start\n",
      "XIII\n",
      "\n",
      "Ah, more with the veil of Love\n",
      " \n",
      "The book of poetry and the cup of wine \n",
      "are your dearest filled -you will me to they cleansed themselves,\n",
      "and a circle of ecstatic wron\n",
      "I can see by a mountains that fire.\n",
      "END\n",
      "Mewlana Jalaluddin Rumi\n",
      "\n",
      "\n",
      "Poem Start\n",
      "If A Tree As Cove Door I Watch Everning,\n",
      " \n",
      "I closed the beauty of this garden for me.\"\n",
      "Every moment is you lips and future,\n",
      "I seek The Cants of Rephyshince my Face of The Beloved's shadow in me \n",
      "The Friend's spendom his ket in this faction, \n",
      "The more pure love, and this impute of gold and silver falsenh through, \n",
      "Yet how should the journey forget my Beloved says to me: \n",
      "And when the Beloved blows streams away from the house wherein this form is, so how\n",
      "should I beguile him with such a form and likeness?\n",
      "He does not take a flock of hor\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "\n",
    "from mingpt.utils import sample\n",
    "trainer.load_checkpoint()\n",
    "context = \"Poem Start\\nPoggers Bro\\n\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0addf8f22d8c836204ce3be080edc45558007cc167fa6a1e84d84279a5c463ed"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
